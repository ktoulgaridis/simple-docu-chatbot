{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kyriakos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/kyriakos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kyriakos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Importing all libraries\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re, string, unicodedata\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import wikipedia as wk\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./hr.txt','r',errors = 'ignore')\n",
    "raw = data.read()\n",
    "raw = raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(text):\n",
    "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "    #word tokenization\n",
    "    word_token = nltk.word_tokenize(text.lower().translate(remove_punct_dict))\n",
    "    \n",
    "    #remove ascii\n",
    "    new_words = []\n",
    "    for word in word_token:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    \n",
    "    #Remove tags\n",
    "    rmv = []\n",
    "    for w in new_words:\n",
    "        text=re.sub(\"&lt;/?.*?&gt;\",\"&lt;&gt;\",w)\n",
    "        rmv.append(text)\n",
    "        \n",
    "    #pos tagging and lemmatization\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemma_list = []\n",
    "    rmv = [i for i in rmv if i]\n",
    "    for token, tag in nltk.pos_tag(rmv):\n",
    "        lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
    "        lemma_list.append(lemma)\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_input = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "welcome_response = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def welcome(user_response):\n",
    "    for word in user_response.split():\n",
    "        if word.lower() in welcome_input:\n",
    "            return random.choice(welcome_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=Normalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    #vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    vals = linear_kernel(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0) or \"tell me about\" in user_response:\n",
    "        print(\"Checking Wikipedia\")\n",
    "        if user_response:\n",
    "            robo_response = wikipedia_data(user_response)\n",
    "            return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "#wikipedia search\n",
    "def wikipedia_data(input):\n",
    "    reg_ex = re.search('tell me about (.*)', input)\n",
    "    try:\n",
    "        if reg_ex:\n",
    "            topic = reg_ex.group(1)\n",
    "            wiki = wk.summary(topic, sentences = 3)\n",
    "            return wiki\n",
    "    except Exception as e:\n",
    "            print(\"No content has been found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\n",
      "Hi\n",
      "Chatterbot : I am glad! You are talking to me\n",
      "Yes I am indeed\n",
      "Chatterbot : and get to a yes!.\n",
      "How are you\n",
      "Chatterbot : Checking Wikipedia\n",
      "None\n",
      "not what i asked\n",
      "Chatterbot : Checking Wikipedia\n",
      "None\n",
      "what is hr\n",
      "Chatterbot : [25] the chief hr officer or hr director is the highest ranking hr executive in most companies.\n",
      "tell me more\n",
      "Chatterbot : whether through their own profiles or through employer branding promotions, they should know how to tell a great story about company.\n",
      "who is the best employee\n",
      "Chatterbot : employee benefits, appraisals, and rewards are all encouragements to bring forward the best employees.\n",
      "bye\n",
      "Chatterbot : Bye!!! \n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"My name is Chatterbot and I'm a chatbot. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response not in ['bye','shutdown','exit', 'quit']):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Chatterbot : You are welcome..\")\n",
    "        else:\n",
    "            if(welcome(user_response)!=None):\n",
    "                print(\"Chatterbot : \"+welcome(user_response))\n",
    "            else:\n",
    "                print(\"Chatterbot : \",end=\"\")\n",
    "                print(generateResponse(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Chatterbot : Bye!!! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
